{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 10\n",
    "BATCH_SIZE = 100\n",
    "INPUT_DIM = 2\n",
    "D_MODEL = 128\n",
    "NHEAD = 2\n",
    "NUM_LAYERS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('S&P_500/AAPL_diff.csv')\n",
    "df = df.drop(columns=['Date', 'Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use forward chaining KFold split strategy, <br>\n",
    "For this sklearn has function called TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "splitter = TimeSeriesSplit(n_splits=N_FOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!!ADD MIN/MAX SCALER!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=80):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].clone().detach()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.transformer_layers = nn.Transformer(d_model, nhead, num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_layers(x, y)\n",
    "        x = self.fc(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MSE loss because we want to penalty our model for big mistakes stronger, because It would lead us to lose money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **remake sliding window/rethink transformer structure and wrap train in mlflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_counter, (indices_train, indices_test) in enumerate(splitter.split(data)):\n",
    "        train_loader = torch.utils.data.DataLoader(data[indices_train],\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   num_workers=4)\n",
    "        test_loader = torch.utils.data.DataLoader(data[indices_test],\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   num_workers=4)\n",
    "        for x in train_loader:\n",
    "            print(x)\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kfolds(data, indices_generator, n_folds, d_model, nhead, num_layers):\n",
    "    for fold_counter, (indices_train, indices_test) in enumerate(indices_generator):\n",
    "        train_loader = torch.utils.data.DataLoader(data[indices_train],\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   num_workers=4)\n",
    "        test_loader = torch.utils.data.DataLoader(data[indices_test],\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   num_workers=4)\n",
    "        #define model\n",
    "        model = SimpleTransformer(input_dim, d_model, nhead, num_layers)\n",
    "    \n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        model.train()\n",
    "        \n",
    "        # Train the model\n",
    "        num_epochs = 100\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model(X_train)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(y_pred, X_train)\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "                \n",
    "        torch.save({'state_dict': model.state_dict(),\n",
    "            'fold_num': fold_counter},\n",
    "           'models/{}_fold_transformer.pth'.format(str(fold_counter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kfolds(data, splitter.split(data), N_FOLDS, D_MODEL, NHEAD, NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after all -- try to eval quality of ensemble of model's trained on different kfolds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
