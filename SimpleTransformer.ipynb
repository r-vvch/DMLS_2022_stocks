{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 10\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "df = pd.read_csv('S&P_500/AAPL_diff.csv')\n",
    "df = df.drop(columns=['Date', 'Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use forward chaining KFold split strategy, <br>\n",
    "For this sklearn has function called TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "splitter = TimeSeriesSplit(n_splits=N_FOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!!ADD MIN/MAX SCALER!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=80):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].clone().detach()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.transformer_layers = nn.Transformer(d_model, nhead, num_layers)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_layers(x, y)\n",
    "        x = self.fc(x)\n",
    "        return x \n",
    "    \n",
    "# Create an instance of the model\n",
    "input_dim = 1\n",
    "d_model = 128\n",
    "nhead = 2\n",
    "num_layers = 3\n",
    "# model = SimpleTransformer(input_dim, d_model, nhead, num_layers)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# # Set the model to training mode\n",
    "# model.train()\n",
    "\n",
    "# # Train the model\n",
    "# num_epochs = 100\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Forward pass\n",
    "#     y_pred = model(X_train)\n",
    "\n",
    "#     # Compute the loss\n",
    "#     loss = criterion(y_pred, X_train)\n",
    "\n",
    "#     # Zero gradients, perform a backward pass, and update the weights\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     if (epoch+1) % 10 == 0:\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use MSE loss because we want to penalty our model for big mistakes stronger, because It would lead us to lose money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.2252e+01,  3.7750e-01],\n",
      "        [ 4.2330e+01,  7.2502e-02],\n",
      "        [ 4.2342e+01, -2.8000e-01],\n",
      "        [ 4.3167e+01,  8.6750e-01],\n",
      "        [ 4.2925e+01, -1.1250e-01],\n",
      "        [ 4.3068e+01, -5.7499e-02],\n",
      "        [ 4.3055e+01, -4.4998e-02],\n",
      "        [ 4.3493e+01,  8.4999e-02],\n",
      "        [ 4.4105e+01,  3.8500e-01],\n",
      "        [ 4.3635e+01, -1.2250e-01],\n",
      "        [ 4.3588e+01, -1.3000e-01],\n",
      "        [ 4.3752e+01,  2.1000e-01],\n",
      "        [ 4.3752e+01,  8.2500e-02],\n",
      "        [ 4.2643e+01, -5.7499e-02],\n",
      "        [ 4.2650e+01,  1.2500e-01],\n",
      "        [ 4.2770e+01,  2.0000e-02],\n",
      "        [ 4.2307e+01, -3.2250e-01],\n",
      "        [ 4.3065e+01,  5.2500e-01],\n",
      "        [ 4.3057e+01, -7.5001e-02],\n",
      "        [ 4.3257e+01,  1.2250e-01],\n",
      "        [ 4.3750e+01,  3.9000e-01],\n",
      "        [ 4.3588e+01,  0.0000e+00],\n",
      "        [ 4.3583e+01, -5.5000e-02],\n",
      "        [ 4.3572e+01,  2.8250e-01],\n",
      "        [ 4.3820e+01,  1.7250e-01],\n",
      "        [ 4.4272e+01,  2.2750e-01],\n",
      "        [ 4.4048e+01, -4.2750e-01],\n",
      "        [ 4.4775e+01,  7.3750e-01],\n",
      "        [ 4.4815e+01, -2.7500e-02],\n",
      "        [ 4.4615e+01, -3.7498e-02],\n",
      "        [ 4.4250e+01, -7.5001e-02],\n",
      "        [ 4.4260e+01, -6.5002e-02],\n",
      "        [ 4.3555e+01, -7.5750e-01],\n",
      "        [ 4.2778e+01, -8.5000e-01],\n",
      "        [ 4.2877e+01, -1.2250e-01],\n",
      "        [ 4.1990e+01, -5.5000e-01],\n",
      "        [ 4.1743e+01,  3.6000e-01],\n",
      "        [ 4.1857e+01,  1.4000e-01],\n",
      "        [ 4.1945e+01,  1.5250e-01],\n",
      "        [ 4.0125e+01, -1.3750e+00],\n",
      "        [ 3.9123e+01, -6.5250e-01],\n",
      "        [ 4.0757e+01,  2.0500e+00],\n",
      "        [ 3.9885e+01, -8.8750e-01],\n",
      "        [ 3.8787e+01, -1.2850e+00],\n",
      "        [ 3.9103e+01, -1.6500e-01],\n",
      "        [ 4.0678e+01,  1.0525e+00],\n",
      "        [ 4.1085e+01,  5.9750e-01],\n",
      "        [ 4.1842e+01,  1.0825e+00],\n",
      "        [ 4.3248e+01,  8.0000e-01],\n",
      "        [ 4.3107e+01,  1.7498e-02],\n",
      "        [ 4.2963e+01, -4.9999e-02],\n",
      "        [ 4.2768e+01, -4.4000e-01],\n",
      "        [ 4.3125e+01,  1.7500e-01],\n",
      "        [ 4.3875e+01,  4.5750e-01],\n",
      "        [ 4.4743e+01,  6.5500e-01],\n",
      "        [ 4.4597e+01, -1.7750e-01],\n",
      "        [ 4.4530e+01, -2.8500e-01],\n",
      "        [ 4.3750e+01, -8.8500e-01],\n",
      "        [ 4.4053e+01,  8.5250e-01],\n",
      "        [ 4.4205e+01,  4.0250e-01],\n",
      "        [ 4.4167e+01, -3.1000e-01],\n",
      "        [ 4.3757e+01,  2.2499e-02],\n",
      "        [ 4.4235e+01,  3.6500e-01],\n",
      "        [ 4.4995e+01,  5.0500e-01],\n",
      "        [ 4.5430e+01,  3.5750e-01],\n",
      "        [ 4.4993e+01, -6.5500e-01],\n",
      "        [ 4.4610e+01, -4.7000e-01],\n",
      "        [ 4.4662e+01,  3.7498e-02],\n",
      "        [ 4.4505e+01, -1.5750e-01],\n",
      "        [ 4.3825e+01, -5.0500e-01],\n",
      "        [ 4.3810e+01,  0.0000e+00],\n",
      "        [ 4.2818e+01, -9.4250e-01],\n",
      "        [ 4.2213e+01, -2.8750e-01],\n",
      "        [ 4.1235e+01, -8.6250e-01],\n",
      "        [ 4.3193e+01,  1.1750e+00],\n",
      "        [ 4.2085e+01, -1.3350e+00],\n",
      "        [ 4.1620e+01, -1.9250e-01],\n",
      "        [ 4.1945e+01, -7.4997e-03],\n",
      "        [ 4.1670e+01,  9.9983e-03],\n",
      "        [ 4.2097e+01,  1.8750e-01],\n",
      "        [ 4.2903e+01,  1.6825e+00],\n",
      "        [ 4.3200e+01,  5.5000e-02],\n",
      "        [ 4.2095e+01, -6.4750e-01],\n",
      "        [ 4.2513e+01,  4.2500e-02],\n",
      "        [ 4.3312e+01,  6.2500e-02],\n",
      "        [ 4.3110e+01,  5.2502e-02],\n",
      "        [ 4.3535e+01,  1.8250e-01],\n",
      "        [ 4.3682e+01, -1.2501e-02],\n",
      "        [ 4.3955e+01,  1.9750e-01],\n",
      "        [ 4.4560e+01,  4.3750e-01],\n",
      "        [ 4.4460e+01,  7.4997e-03],\n",
      "        [ 4.3200e+01, -2.4000e-01],\n",
      "        [ 4.1430e+01, -1.2200e+00],\n",
      "        [ 4.1310e+01, -3.9750e-01],\n",
      "        [ 4.0735e+01, -6.8250e-01],\n",
      "        [ 4.0912e+01,  2.5750e-01],\n",
      "        [ 4.1055e+01,  2.5002e-02],\n",
      "        [ 4.0580e+01, -4.2000e-01],\n",
      "        [ 4.1315e+01,  7.8250e-01],\n",
      "        [ 4.2275e+01,  6.7250e-01]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for fold_counter, (indices_train, indices_test) in enumerate(splitter.split(data)):\n",
    "        train_loader = torch.utils.data.DataLoader(data[indices_train],\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   num_workers=4)\n",
    "        test_loader = torch.utils.data.DataLoader(data[indices_test],\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   num_workers=4)\n",
    "        for x in train_loader:\n",
    "            print(x)\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kfolds(data, indices_generator, n_folds, d_model, nhead, num_layers):\n",
    "    for fold_counter, (indices_train, indices_test) in enumerate(indices_generator):\n",
    "        train_loader = torch.utils.data.DataLoader(data[indices_train],\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   num_workers=4)\n",
    "        test_loader = torch.utils.data.DataLoader(data[indices_test],\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   num_workers=4)\n",
    "        #define model\n",
    "        model = SimpleTransformer(input_dim, d_model, nhead, num_layers)\n",
    "    \n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        model.train()\n",
    "        \n",
    "        # Train the model\n",
    "        num_epochs = 100\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model(X_train)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(y_pred, X_train)\n",
    "\n",
    "            # Zero gradients, perform a backward pass, and update the weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "                \n",
    "        torch.save({'state_dict': model.state_dict(),\n",
    "            'fold_num': fold_counter},\n",
    "           'models/{}_fold_transformer.pth'.format(str(fold_counter)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kfolds(data, splitter.split(data), N_FOLDS, d_model, nhead, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
